{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b83f0ad8-a405-4d37-9ee3-3bcf2ff85fd9",
   "metadata": {
    "id": "b83f0ad8-a405-4d37-9ee3-3bcf2ff85fd9"
   },
   "outputs": [],
   "source": [
    "# Libraries needed, may need to do a \"pip3 install [package-name]\"\n",
    "# To install jupyer lab: \"pip3 install jupyterlab\"\n",
    "# \"pip3 install jupyterlab torch transformers pandas scikit-learn\"\n",
    "# Launching jupyter lab: \"jupyter lab\"\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a78c483d-31ff-418e-bcfd-4c364384a5cc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a78c483d-31ff-418e-bcfd-4c364384a5cc",
    "outputId": "0fb50378-6a0a-48ee-9f34-9bf0bcb38a70"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Splitting the data and tokenizing it\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/Train.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(data\u001b[38;5;241m.\u001b[39mhead())\n\u001b[0;32m      5\u001b[0m X_train, X_test, Y_train, Y_test \u001b[38;5;241m=\u001b[39m train_test_split(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m], data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m], train_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Splitting the data and tokenizing it\n",
    "data = pd.read_csv(\"data/Train.csv\")\n",
    "print(data.head())\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data['text'], data['target'], train_size=0.2, shuffle=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "train_tokens = tokenizer(list(X_train), padding=True, truncation=True)\n",
    "test_tokens = tokenizer(list(X_test), padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909f4a98-f736-42a9-8335-fd8ea25cfd06",
   "metadata": {
    "id": "909f4a98-f736-42a9-8335-fd8ea25cfd06"
   },
   "outputs": [],
   "source": [
    "# This class allows the data to be formatted correctly with the DataLoader in PyTorch.\n",
    "# We don't need this, but it simplfies the process a lot.\n",
    "# DataLoader is a utility that simplifies loading and managing datasets, especially large ones.\n",
    "# DataLoader allows us to use batches, which means we can group multiple inputs together instead of\n",
    "# doing this one by one which is incredibly time saving.\n",
    "class TokenData(Dataset):\n",
    "    def __init__(self, train = False):\n",
    "        if train:\n",
    "            self.text_data = X_train\n",
    "            self.tokens = train_tokens\n",
    "            self.labels = list(Y_train)\n",
    "        else:\n",
    "            self.text_data = X_test\n",
    "            self.tokens = test_tokens\n",
    "            self.labels = list(Y_test)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {}\n",
    "        for k, v in self.tokens.items():\n",
    "            sample[k] = torch.tensor(v[idx])\n",
    "        sample['labels'] = torch.tensor(self.labels[idx])\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a690722-4d79-4d4f-aac4-4f69feab5f24",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1a690722-4d79-4d4f-aac4-4f69feab5f24",
    "outputId": "a6b1ec65-f7bc-4849-de11-f8b0507faeba"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the DataLoader, Model, Optimizer, and Loss function\n",
    "batch_size = 8\n",
    "\n",
    "train_dataset = TokenData(train=True)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "test_dataset = TokenData(train=False)\n",
    "test_loader = DataLoader(test_dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "bert_model = BertForSequenceClassification.from_pretrained(\"bert-base-cased\")\n",
    "optimizer = AdamW(bert_model.parameters(), lr=1e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 1\n",
    "device = \"cpu\"\n",
    "bert_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288a9525-de18-4cf4-bfb9-09f2cc5f0288",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "288a9525-de18-4cf4-bfb9-09f2cc5f0288",
    "outputId": "9e8a7c9f-9473-44e3-dcbf-31dd67b0d247"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "Training batch 1 last loss: 0.08928822726011276\n",
      "Training batch 2 last loss: 0.08651192486286163\n",
      "Training batch 3 last loss: 0.0942913293838501\n",
      "Training batch 4 last loss: 0.0868922546505928\n",
      "Training batch 5 last loss: 0.08840437233448029\n",
      "Training batch 6 last loss: 0.08826050162315369\n",
      "Training batch 7 last loss: 0.08297395706176758\n",
      "Training batch 8 last loss: 0.08706354349851608\n",
      "Training batch 9 last loss: 0.08159544318914413\n",
      "Training batch 10 last loss: 0.08254213631153107\n",
      "Training batch 11 last loss: 0.08098576217889786\n",
      "Training batch 12 last loss: 0.08731159567832947\n",
      "Training batch 13 last loss: 0.0872943177819252\n",
      "Training batch 14 last loss: 0.08410470932722092\n",
      "Training batch 15 last loss: 0.07968933135271072\n",
      "Training batch 16 last loss: 0.08732444047927856\n",
      "Training batch 17 last loss: 0.08102112263441086\n",
      "Training batch 18 last loss: 0.0789869949221611\n",
      "Training batch 19 last loss: 0.091135673224926\n",
      "Training batch 20 last loss: 0.0795910432934761\n",
      "Training batch 21 last loss: 0.07844409346580505\n",
      "Training batch 22 last loss: 0.08682506531476974\n",
      "Training batch 23 last loss: 0.07933662831783295\n",
      "Training batch 24 last loss: 0.08042000979185104\n",
      "Training batch 25 last loss: 0.08399534225463867\n",
      "Training batch 26 last loss: 0.08634413778781891\n",
      "Training batch 27 last loss: 0.07558983564376831\n",
      "Training batch 28 last loss: 0.08451615273952484\n",
      "Training batch 29 last loss: 0.09958802163600922\n",
      "Training batch 30 last loss: 0.08687835186719894\n",
      "Training batch 31 last loss: 0.08813360333442688\n",
      "Training batch 32 last loss: 0.0758216455578804\n",
      "Training batch 33 last loss: 0.07453649491071701\n",
      "Training batch 34 last loss: 0.08741891384124756\n",
      "Training batch 35 last loss: 0.0836157500743866\n",
      "Training batch 36 last loss: 0.08307856321334839\n",
      "Training batch 37 last loss: 0.08441346883773804\n",
      "Training batch 38 last loss: 0.0913769006729126\n",
      "Training batch 39 last loss: 0.07192893326282501\n",
      "Training batch 40 last loss: 0.07266930490732193\n",
      "Training batch 41 last loss: 0.07597753405570984\n",
      "Training batch 42 last loss: 0.07532881945371628\n",
      "Training batch 43 last loss: 0.07694535702466965\n",
      "Training batch 44 last loss: 0.07450348138809204\n",
      "Training batch 45 last loss: 0.06771929562091827\n",
      "Training batch 46 last loss: 0.07944706082344055\n",
      "Training batch 47 last loss: 0.08127422630786896\n",
      "Training batch 48 last loss: 0.08458803594112396\n",
      "Training batch 49 last loss: 0.0833936482667923\n",
      "Training batch 50 last loss: 0.060980916023254395\n",
      "Training batch 51 last loss: 0.07362344861030579\n",
      "Training batch 52 last loss: 0.0878513753414154\n",
      "Training batch 53 last loss: 0.06400330364704132\n",
      "Training batch 54 last loss: 0.06978069990873337\n",
      "Training batch 55 last loss: 0.07236319035291672\n",
      "Training batch 56 last loss: 0.07069790363311768\n",
      "Training batch 57 last loss: 0.05607438459992409\n",
      "Training batch 58 last loss: 0.0791655033826828\n",
      "Training batch 59 last loss: 0.07437459379434586\n",
      "Training batch 60 last loss: 0.08929569274187088\n",
      "Training batch 61 last loss: 0.07296465337276459\n",
      "Training batch 62 last loss: 0.07447488605976105\n",
      "Training batch 63 last loss: 0.059723496437072754\n",
      "Training batch 64 last loss: 0.059229906648397446\n",
      "Training batch 65 last loss: 0.05733141675591469\n",
      "Training batch 66 last loss: 0.0769747644662857\n",
      "Training batch 67 last loss: 0.04527309536933899\n",
      "Training batch 68 last loss: 0.055453136563301086\n",
      "Training batch 69 last loss: 0.06544939428567886\n",
      "Training batch 70 last loss: 0.04292580857872963\n",
      "Training batch 71 last loss: 0.0816846638917923\n",
      "Training batch 72 last loss: 0.07664034515619278\n",
      "Training batch 73 last loss: 0.05357407033443451\n",
      "Training batch 74 last loss: 0.0680622011423111\n",
      "Training batch 75 last loss: 0.04945937171578407\n",
      "Training batch 76 last loss: 0.07205687463283539\n",
      "Training batch 77 last loss: 0.07781434059143066\n",
      "Training batch 78 last loss: 0.08192168176174164\n",
      "Training batch 79 last loss: 0.03991842269897461\n",
      "Training batch 80 last loss: 0.05687294900417328\n",
      "Training batch 81 last loss: 0.05915842205286026\n",
      "Training batch 82 last loss: 0.07227208465337753\n",
      "Training batch 83 last loss: 0.04293324798345566\n",
      "Training batch 84 last loss: 0.04072345048189163\n",
      "Training batch 85 last loss: 0.044699858874082565\n",
      "Training batch 86 last loss: 0.04034549370408058\n",
      "Training batch 87 last loss: 0.05304448679089546\n",
      "Training batch 88 last loss: 0.0542781837284565\n",
      "Training batch 89 last loss: 0.050913549959659576\n",
      "Training batch 90 last loss: 0.023421168327331543\n",
      "Training batch 91 last loss: 0.04839014634490013\n",
      "Training batch 92 last loss: 0.028186902403831482\n",
      "Training batch 93 last loss: 0.0389961376786232\n",
      "Training batch 94 last loss: 0.03415195271372795\n",
      "Training batch 95 last loss: 0.06960650533437729\n",
      "Training batch 96 last loss: 0.06437736004590988\n",
      "Training batch 97 last loss: 0.0290974248200655\n",
      "Training batch 98 last loss: 0.044068094342947006\n",
      "Training batch 99 last loss: 0.03402743861079216\n",
      "Training batch 100 last loss: 0.052346497774124146\n",
      "Training batch 101 last loss: 0.05390291288495064\n",
      "Training batch 102 last loss: 0.02391854301095009\n",
      "Training batch 103 last loss: 0.08042410761117935\n",
      "Training batch 104 last loss: 0.07690369337797165\n",
      "Training batch 105 last loss: 0.04501703009009361\n",
      "Training batch 106 last loss: 0.04526161774992943\n",
      "Training batch 107 last loss: 0.041172318160533905\n",
      "Training batch 108 last loss: 0.03538665920495987\n",
      "Training batch 109 last loss: 0.06930597126483917\n",
      "Training batch 110 last loss: 0.04100044071674347\n",
      "Training batch 111 last loss: 0.0236772783100605\n",
      "Training batch 112 last loss: 0.07063771784305573\n",
      "Training batch 113 last loss: 0.02131384238600731\n",
      "Training batch 114 last loss: 0.06385878473520279\n",
      "Training batch 115 last loss: 0.0551171600818634\n",
      "Training batch 116 last loss: 0.01668066345155239\n",
      "Training batch 117 last loss: 0.02036178484559059\n",
      "Training batch 118 last loss: 0.07089120894670486\n",
      "Training batch 119 last loss: 0.01261186320334673\n",
      "Training batch 120 last loss: 0.08935096114873886\n",
      "Training batch 121 last loss: 0.054998259991407394\n",
      "Training batch 122 last loss: 0.016647998243570328\n",
      "Training batch 123 last loss: 0.07159816473722458\n",
      "Training batch 124 last loss: 0.013840832747519016\n",
      "Training batch 125 last loss: 0.09300248324871063\n",
      "Training batch 126 last loss: 0.05518408864736557\n",
      "Training batch 127 last loss: 0.042127564549446106\n",
      "Training batch 128 last loss: 0.02437315136194229\n",
      "Training batch 129 last loss: 0.06437306106090546\n",
      "Training batch 130 last loss: 0.05861614644527435\n",
      "Training batch 131 last loss: 0.04212392494082451\n",
      "Training batch 132 last loss: 0.05209338292479515\n",
      "Training batch 133 last loss: 0.014669647440314293\n",
      "\n",
      "Training epoch 1 loss:  0.014669647440314293\n",
      "Testing batch 1 loss: 0.036035340279340744\n",
      "Testing accuracy:  0.75\n",
      "Testing batch 2 loss: 0.04059608280658722\n",
      "Testing accuracy:  0.8125\n",
      "Testing batch 3 loss: 0.050121963024139404\n",
      "Testing accuracy:  0.8333333333333334\n",
      "Testing batch 4 loss: 0.08817997574806213\n",
      "Testing accuracy:  0.78125\n",
      "Testing batch 5 loss: 0.08765199780464172\n",
      "Testing accuracy:  0.75\n",
      "Testing batch 6 loss: 0.0889451801776886\n",
      "Testing accuracy:  0.75\n",
      "Testing batch 7 loss: 0.05632708594202995\n",
      "Testing accuracy:  0.75\n",
      "Testing batch 8 loss: 0.040757227689027786\n",
      "Testing accuracy:  0.765625\n",
      "Testing batch 9 loss: 0.021666672080755234\n",
      "Testing accuracy:  0.7916666666666666\n",
      "Testing batch 10 loss: 0.03873137757182121\n",
      "Testing accuracy:  0.8\n",
      "Testing batch 11 loss: 0.029987191781401634\n",
      "Testing accuracy:  0.8068181818181818\n",
      "Testing batch 12 loss: 0.039599619805812836\n",
      "Testing accuracy:  0.8125\n",
      "Testing batch 13 loss: 0.01432675588876009\n",
      "Testing accuracy:  0.8269230769230769\n",
      "Testing batch 14 loss: 0.04921376332640648\n",
      "Testing accuracy:  0.8303571428571429\n",
      "Testing batch 15 loss: 0.04596853256225586\n",
      "Testing accuracy:  0.8333333333333334\n",
      "Testing batch 16 loss: 0.11601048707962036\n",
      "Testing accuracy:  0.8046875\n",
      "Testing batch 17 loss: 0.053854674100875854\n",
      "Testing accuracy:  0.7941176470588235\n",
      "Testing batch 18 loss: 0.04197121039032936\n",
      "Testing accuracy:  0.7986111111111112\n",
      "Testing batch 19 loss: 0.037387166172266006\n",
      "Testing accuracy:  0.8092105263157895\n",
      "Testing batch 20 loss: 0.05400838330388069\n",
      "Testing accuracy:  0.80625\n",
      "Testing batch 21 loss: 0.06428951025009155\n",
      "Testing accuracy:  0.7976190476190477\n",
      "Testing batch 22 loss: 0.020298859104514122\n",
      "Testing accuracy:  0.8068181818181818\n",
      "Testing batch 23 loss: 0.10311490297317505\n",
      "Testing accuracy:  0.7989130434782609\n",
      "Testing batch 24 loss: 0.027956422418355942\n",
      "Testing accuracy:  0.8072916666666666\n",
      "Testing batch 25 loss: 0.06081877648830414\n",
      "Testing accuracy:  0.8\n",
      "Testing batch 26 loss: 0.028081100434064865\n",
      "Testing accuracy:  0.8076923076923077\n",
      "Testing batch 27 loss: 0.055474210530519485\n",
      "Testing accuracy:  0.8101851851851852\n",
      "Testing batch 28 loss: 0.04789792001247406\n",
      "Testing accuracy:  0.8169642857142857\n",
      "Testing batch 29 loss: 0.06717497855424881\n",
      "Testing accuracy:  0.8146551724137931\n",
      "Testing batch 30 loss: 0.09034662693738937\n",
      "Testing accuracy:  0.8083333333333333\n",
      "Testing batch 31 loss: 0.03378259018063545\n",
      "Testing accuracy:  0.8104838709677419\n",
      "Testing batch 32 loss: 0.019036056473851204\n",
      "Testing accuracy:  0.81640625\n",
      "Testing batch 33 loss: 0.04805050790309906\n",
      "Testing accuracy:  0.8181818181818182\n",
      "Testing batch 34 loss: 0.04576314613223076\n",
      "Testing accuracy:  0.8198529411764706\n",
      "Testing batch 35 loss: 0.05074768140912056\n",
      "Testing accuracy:  0.8178571428571428\n",
      "Testing batch 36 loss: 0.08760808408260345\n",
      "Testing accuracy:  0.8125\n",
      "Testing batch 37 loss: 0.04710356146097183\n",
      "Testing accuracy:  0.8108108108108109\n",
      "Testing batch 38 loss: 0.08169618993997574\n",
      "Testing accuracy:  0.8125\n",
      "Testing batch 39 loss: 0.03984210640192032\n",
      "Testing accuracy:  0.8173076923076923\n",
      "Testing batch 40 loss: 0.03954780846834183\n",
      "Testing accuracy:  0.821875\n",
      "Testing batch 41 loss: 0.058927737176418304\n",
      "Testing accuracy:  0.823170731707317\n",
      "Testing batch 42 loss: 0.026708504185080528\n",
      "Testing accuracy:  0.8273809523809523\n",
      "Testing batch 43 loss: 0.0681297555565834\n",
      "Testing accuracy:  0.8284883720930233\n",
      "Testing batch 44 loss: 0.024337327107787132\n",
      "Testing accuracy:  0.8323863636363636\n",
      "Testing batch 45 loss: 0.08759890496730804\n",
      "Testing accuracy:  0.8333333333333334\n",
      "Testing batch 46 loss: 0.051443081349134445\n",
      "Testing accuracy:  0.8342391304347826\n",
      "Testing batch 47 loss: 0.0468103252351284\n",
      "Testing accuracy:  0.8351063829787234\n",
      "Testing batch 48 loss: 0.04202684760093689\n",
      "Testing accuracy:  0.8359375\n",
      "Testing batch 49 loss: 0.05131973698735237\n",
      "Testing accuracy:  0.8341836734693877\n",
      "Testing batch 50 loss: 0.03252502903342247\n",
      "Testing accuracy:  0.8375\n",
      "Testing batch 51 loss: 0.060352787375450134\n",
      "Testing accuracy:  0.8357843137254902\n",
      "Testing batch 52 loss: 0.05700577050447464\n",
      "Testing accuracy:  0.8341346153846154\n",
      "Testing batch 53 loss: 0.03187839686870575\n",
      "Testing accuracy:  0.8372641509433962\n",
      "Testing batch 54 loss: 0.025367675349116325\n",
      "Testing accuracy:  0.8402777777777778\n",
      "Testing batch 55 loss: 0.0449630469083786\n",
      "Testing accuracy:  0.8409090909090909\n",
      "Testing batch 56 loss: 0.03114238753914833\n",
      "Testing accuracy:  0.84375\n",
      "Testing batch 57 loss: 0.04996094852685928\n",
      "Testing accuracy:  0.8442982456140351\n",
      "Testing batch 58 loss: 0.05931415408849716\n",
      "Testing accuracy:  0.8426724137931034\n",
      "Testing batch 59 loss: 0.03317902609705925\n",
      "Testing accuracy:  0.8453389830508474\n",
      "Testing batch 60 loss: 0.05442560464143753\n",
      "Testing accuracy:  0.8458333333333333\n",
      "Testing batch 61 loss: 0.05411114543676376\n",
      "Testing accuracy:  0.8442622950819673\n",
      "Testing batch 62 loss: 0.07566290348768234\n",
      "Testing accuracy:  0.8407258064516129\n",
      "Testing batch 63 loss: 0.04835480824112892\n",
      "Testing accuracy:  0.8412698412698413\n",
      "Testing batch 64 loss: 0.03281256556510925\n",
      "Testing accuracy:  0.841796875\n",
      "Testing batch 65 loss: 0.02057190053164959\n",
      "Testing accuracy:  0.8442307692307692\n",
      "Testing batch 66 loss: 0.03655163571238518\n",
      "Testing accuracy:  0.8446969696969697\n",
      "Testing batch 67 loss: 0.08634782582521439\n",
      "Testing accuracy:  0.8432835820895522\n",
      "Testing batch 68 loss: 0.08044157922267914\n",
      "Testing accuracy:  0.8400735294117647\n",
      "Testing batch 69 loss: 0.05489494651556015\n",
      "Testing accuracy:  0.8405797101449275\n",
      "Testing batch 70 loss: 0.048452429473400116\n",
      "Testing accuracy:  0.8410714285714286\n",
      "Testing batch 71 loss: 0.017233185470104218\n",
      "Testing accuracy:  0.8433098591549296\n",
      "Testing batch 72 loss: 0.042807437479496\n",
      "Testing accuracy:  0.84375\n",
      "Testing batch 73 loss: 0.054860349744558334\n",
      "Testing accuracy:  0.8424657534246576\n",
      "Testing batch 74 loss: 0.07301786541938782\n",
      "Testing accuracy:  0.839527027027027\n",
      "Testing batch 75 loss: 0.02796192839741707\n",
      "Testing accuracy:  0.8416666666666667\n",
      "Testing batch 76 loss: 0.04393499717116356\n",
      "Testing accuracy:  0.8421052631578947\n",
      "Testing batch 77 loss: 0.031737834215164185\n",
      "Testing accuracy:  0.8441558441558441\n",
      "Testing batch 78 loss: 0.030722733587026596\n",
      "Testing accuracy:  0.844551282051282\n",
      "Testing batch 79 loss: 0.04120936617255211\n",
      "Testing accuracy:  0.8465189873417721\n",
      "Testing batch 80 loss: 0.051329415291547775\n",
      "Testing accuracy:  0.846875\n",
      "Testing batch 81 loss: 0.02678394690155983\n",
      "Testing accuracy:  0.8487654320987654\n",
      "Testing batch 82 loss: 0.03387854993343353\n",
      "Testing accuracy:  0.850609756097561\n",
      "Testing batch 83 loss: 0.07282045483589172\n",
      "Testing accuracy:  0.8509036144578314\n",
      "Testing batch 84 loss: 0.014772925525903702\n",
      "Testing accuracy:  0.8526785714285714\n",
      "Testing batch 85 loss: 0.04392296448349953\n",
      "Testing accuracy:  0.8529411764705882\n",
      "Testing batch 86 loss: 0.016152719035744667\n",
      "Testing accuracy:  0.8546511627906976\n",
      "Testing batch 87 loss: 0.045518241822719574\n",
      "Testing accuracy:  0.853448275862069\n",
      "Testing batch 88 loss: 0.04335438087582588\n",
      "Testing accuracy:  0.8508522727272727\n",
      "Testing batch 89 loss: 0.05462489649653435\n",
      "Testing accuracy:  0.851123595505618\n",
      "Testing batch 90 loss: 0.054842494428157806\n",
      "Testing accuracy:  0.85\n",
      "Testing batch 91 loss: 0.053650062531232834\n",
      "Testing accuracy:  0.8489010989010989\n",
      "Testing batch 92 loss: 0.03637735918164253\n",
      "Testing accuracy:  0.8505434782608695\n",
      "Testing batch 93 loss: 0.04300829395651817\n",
      "Testing accuracy:  0.8508064516129032\n",
      "Testing batch 94 loss: 0.05328746140003204\n",
      "Testing accuracy:  0.8497340425531915\n",
      "Testing batch 95 loss: 0.03698701038956642\n",
      "Testing accuracy:  0.85\n",
      "Testing batch 96 loss: 0.03628497198224068\n",
      "Testing accuracy:  0.8515625\n",
      "Testing batch 97 loss: 0.030189719051122665\n",
      "Testing accuracy:  0.8530927835051546\n",
      "Testing batch 98 loss: 0.047041069716215134\n",
      "Testing accuracy:  0.8520408163265306\n",
      "Testing batch 99 loss: 0.08448929339647293\n",
      "Testing accuracy:  0.851010101010101\n",
      "Testing batch 100 loss: 0.059406813234090805\n",
      "Testing accuracy:  0.85\n",
      "Testing batch 101 loss: 0.019864019006490707\n",
      "Testing accuracy:  0.8514851485148515\n",
      "Testing batch 102 loss: 0.060999274253845215\n",
      "Testing accuracy:  0.8504901960784313\n",
      "Testing batch 103 loss: 0.04598003625869751\n",
      "Testing accuracy:  0.8507281553398058\n",
      "Testing batch 104 loss: 0.03746766597032547\n",
      "Testing accuracy:  0.8521634615384616\n",
      "Testing batch 105 loss: 0.07404470443725586\n",
      "Testing accuracy:  0.8523809523809524\n",
      "Testing batch 106 loss: 0.03692517429590225\n",
      "Testing accuracy:  0.8525943396226415\n",
      "Testing batch 107 loss: 0.035588040947914124\n",
      "Testing accuracy:  0.8539719626168224\n",
      "Testing batch 108 loss: 0.09927327930927277\n",
      "Testing accuracy:  0.8518518518518519\n",
      "Testing batch 109 loss: 0.031605467200279236\n",
      "Testing accuracy:  0.8532110091743119\n",
      "Testing batch 110 loss: 0.015186097472906113\n",
      "Testing accuracy:  0.8545454545454545\n",
      "Testing batch 111 loss: 0.04034649208188057\n",
      "Testing accuracy:  0.8547297297297297\n",
      "Testing batch 112 loss: 0.03359566628932953\n",
      "Testing accuracy:  0.8549107142857143\n",
      "Testing batch 113 loss: 0.060199134051799774\n",
      "Testing accuracy:  0.8550884955752213\n",
      "Testing batch 114 loss: 0.04265427216887474\n",
      "Testing accuracy:  0.8541666666666666\n",
      "Testing batch 115 loss: 0.022543836385011673\n",
      "Testing accuracy:  0.8554347826086957\n",
      "Testing batch 116 loss: 0.03614361956715584\n",
      "Testing accuracy:  0.8566810344827587\n",
      "Testing batch 117 loss: 0.02947082929313183\n",
      "Testing accuracy:  0.8568376068376068\n",
      "Testing batch 118 loss: 0.06531397253274918\n",
      "Testing accuracy:  0.8569915254237288\n",
      "Testing batch 119 loss: 0.06620562076568604\n",
      "Testing accuracy:  0.8560924369747899\n",
      "Testing batch 120 loss: 0.015609422698616982\n",
      "Testing accuracy:  0.8572916666666667\n",
      "Testing batch 121 loss: 0.07043839246034622\n",
      "Testing accuracy:  0.856404958677686\n",
      "Testing batch 122 loss: 0.04094591736793518\n",
      "Testing accuracy:  0.8565573770491803\n",
      "Testing batch 123 loss: 0.07038179785013199\n",
      "Testing accuracy:  0.8567073170731707\n",
      "Testing batch 124 loss: 0.037477508187294006\n",
      "Testing accuracy:  0.8568548387096774\n",
      "Testing batch 125 loss: 0.034534893929958344\n",
      "Testing accuracy:  0.858\n",
      "Testing batch 126 loss: 0.06444784998893738\n",
      "Testing accuracy:  0.8571428571428571\n",
      "Testing batch 127 loss: 0.045439280569553375\n",
      "Testing accuracy:  0.8572834645669292\n",
      "Testing batch 128 loss: 0.022758373990654945\n",
      "Testing accuracy:  0.8583984375\n",
      "Testing batch 129 loss: 0.03688300400972366\n",
      "Testing accuracy:  0.8594961240310077\n",
      "Testing batch 130 loss: 0.023361489176750183\n",
      "Testing accuracy:  0.8605769230769231\n",
      "Testing batch 131 loss: 0.02614656835794449\n",
      "Testing accuracy:  0.8616412213740458\n",
      "Testing batch 132 loss: 0.053268205374479294\n",
      "Testing accuracy:  0.8617424242424242\n",
      "Testing batch 133 loss: 0.025539521127939224\n",
      "Testing accuracy:  0.8627819548872181\n",
      "Testing batch 134 loss: 0.05868469923734665\n",
      "Testing accuracy:  0.8628731343283582\n",
      "Testing batch 135 loss: 0.04418868571519852\n",
      "Testing accuracy:  0.8629629629629629\n",
      "Testing batch 136 loss: 0.06195361539721489\n",
      "Testing accuracy:  0.8630514705882353\n",
      "Testing batch 137 loss: 0.028250083327293396\n",
      "Testing accuracy:  0.8631386861313869\n",
      "Testing batch 138 loss: 0.03763499855995178\n",
      "Testing accuracy:  0.8632246376811594\n",
      "Testing batch 139 loss: 0.03927304223179817\n",
      "Testing accuracy:  0.8633093525179856\n",
      "Testing batch 140 loss: 0.040193624794483185\n",
      "Testing accuracy:  0.8633928571428572\n",
      "Testing batch 141 loss: 0.07626064121723175\n",
      "Testing accuracy:  0.8634751773049646\n",
      "Testing batch 142 loss: 0.0721442848443985\n",
      "Testing accuracy:  0.8626760563380281\n",
      "Testing batch 143 loss: 0.06194314733147621\n",
      "Testing accuracy:  0.8618881118881119\n",
      "Testing batch 144 loss: 0.07045070081949234\n",
      "Testing accuracy:  0.859375\n",
      "Testing batch 145 loss: 0.03923162445425987\n",
      "Testing accuracy:  0.8603448275862069\n",
      "Testing batch 146 loss: 0.04979466646909714\n",
      "Testing accuracy:  0.860445205479452\n",
      "Testing batch 147 loss: 0.07203976809978485\n",
      "Testing accuracy:  0.8579931972789115\n",
      "Testing batch 148 loss: 0.054103344678878784\n",
      "Testing accuracy:  0.8572635135135135\n",
      "Testing batch 149 loss: 0.02582019381225109\n",
      "Testing accuracy:  0.8582214765100671\n",
      "Testing batch 150 loss: 0.0711037889122963\n",
      "Testing accuracy:  0.8583333333333333\n",
      "Testing batch 151 loss: 0.05685914307832718\n",
      "Testing accuracy:  0.8576158940397351\n",
      "Testing batch 152 loss: 0.06145866960287094\n",
      "Testing accuracy:  0.8569078947368421\n",
      "Testing batch 153 loss: 0.07362663000822067\n",
      "Testing accuracy:  0.8553921568627451\n",
      "Testing batch 154 loss: 0.03399287536740303\n",
      "Testing accuracy:  0.8555194805194806\n",
      "Testing batch 155 loss: 0.033999256789684296\n",
      "Testing accuracy:  0.8564516129032258\n",
      "Testing batch 156 loss: 0.03517602011561394\n",
      "Testing accuracy:  0.8573717948717948\n",
      "Testing batch 157 loss: 0.05287078768014908\n",
      "Testing accuracy:  0.857484076433121\n",
      "Testing batch 158 loss: 0.06101395562291145\n",
      "Testing accuracy:  0.8575949367088608\n",
      "Testing batch 159 loss: 0.07359195500612259\n",
      "Testing accuracy:  0.8577044025157232\n",
      "Testing batch 160 loss: 0.060716886073350906\n",
      "Testing accuracy:  0.85625\n",
      "Testing batch 161 loss: 0.03886779770255089\n",
      "Testing accuracy:  0.8571428571428571\n",
      "Testing batch 162 loss: 0.040069665759801865\n",
      "Testing accuracy:  0.8572530864197531\n",
      "Testing batch 163 loss: 0.10981997102499008\n",
      "Testing accuracy:  0.8558282208588958\n",
      "Testing batch 164 loss: 0.05885658413171768\n",
      "Testing accuracy:  0.8551829268292683\n",
      "Testing batch 165 loss: 0.026546038687229156\n",
      "Testing accuracy:  0.8560606060606061\n",
      "Testing batch 166 loss: 0.023090358823537827\n",
      "Testing accuracy:  0.8569277108433735\n",
      "Testing batch 167 loss: 0.04933621361851692\n",
      "Testing accuracy:  0.8570359281437125\n",
      "Testing batch 168 loss: 0.0730770155787468\n",
      "Testing accuracy:  0.8556547619047619\n",
      "Testing batch 169 loss: 0.022416777908802032\n",
      "Testing accuracy:  0.856508875739645\n",
      "Testing batch 170 loss: 0.06687857210636139\n",
      "Testing accuracy:  0.8566176470588235\n",
      "Testing batch 171 loss: 0.023101184517145157\n",
      "Testing accuracy:  0.8574561403508771\n",
      "Testing batch 172 loss: 0.029305357486009598\n",
      "Testing accuracy:  0.8575581395348837\n",
      "Testing batch 173 loss: 0.03092609904706478\n",
      "Testing accuracy:  0.8583815028901735\n",
      "Testing batch 174 loss: 0.056884765625\n",
      "Testing accuracy:  0.8584770114942529\n",
      "Testing batch 175 loss: 0.06524104624986649\n",
      "Testing accuracy:  0.8578571428571429\n",
      "Testing batch 176 loss: 0.048171866685152054\n",
      "Testing accuracy:  0.8579545454545454\n",
      "Testing batch 177 loss: 0.04990113526582718\n",
      "Testing accuracy:  0.8580508474576272\n",
      "Testing batch 178 loss: 0.02664615586400032\n",
      "Testing accuracy:  0.8588483146067416\n",
      "Testing batch 179 loss: 0.06548198312520981\n",
      "Testing accuracy:  0.8589385474860335\n",
      "Testing batch 180 loss: 0.04096429422497749\n",
      "Testing accuracy:  0.8583333333333333\n",
      "Testing batch 181 loss: 0.02566034346818924\n",
      "Testing accuracy:  0.8584254143646409\n",
      "Testing batch 182 loss: 0.10040119290351868\n",
      "Testing accuracy:  0.8571428571428571\n",
      "Testing batch 183 loss: 0.06395211815834045\n",
      "Testing accuracy:  0.85724043715847\n",
      "Testing batch 184 loss: 0.07944949716329575\n",
      "Testing accuracy:  0.8559782608695652\n",
      "Testing batch 185 loss: 0.03825746476650238\n",
      "Testing accuracy:  0.856081081081081\n",
      "Testing batch 186 loss: 0.03785471245646477\n",
      "Testing accuracy:  0.8568548387096774\n",
      "Testing batch 187 loss: 0.0625961646437645\n",
      "Testing accuracy:  0.8562834224598931\n",
      "Testing batch 188 loss: 0.06060253828763962\n",
      "Testing accuracy:  0.855718085106383\n",
      "Testing batch 189 loss: 0.06557439267635345\n",
      "Testing accuracy:  0.8551587301587301\n",
      "Testing batch 190 loss: 0.04912330210208893\n",
      "Testing accuracy:  0.8546052631578948\n",
      "Testing batch 191 loss: 0.03962705284357071\n",
      "Testing accuracy:  0.8547120418848168\n",
      "Testing batch 192 loss: 0.027717415243387222\n",
      "Testing accuracy:  0.8548177083333334\n",
      "Testing batch 193 loss: 0.04567742347717285\n",
      "Testing accuracy:  0.8542746113989638\n",
      "Testing batch 194 loss: 0.07605789601802826\n",
      "Testing accuracy:  0.8530927835051546\n",
      "Testing batch 195 loss: 0.06255780160427094\n",
      "Testing accuracy:  0.8525641025641025\n",
      "Testing batch 196 loss: 0.07688628137111664\n",
      "Testing accuracy:  0.8520408163265306\n",
      "Testing batch 197 loss: 0.04400729387998581\n",
      "Testing accuracy:  0.8515228426395939\n",
      "Testing batch 198 loss: 0.09877442568540573\n",
      "Testing accuracy:  0.851010101010101\n",
      "Testing batch 199 loss: 0.017034942284226418\n",
      "Testing accuracy:  0.8517587939698492\n",
      "Testing batch 200 loss: 0.045936860144138336\n",
      "Testing accuracy:  0.85125\n",
      "Testing batch 201 loss: 0.03374331444501877\n",
      "Testing accuracy:  0.8519900497512438\n",
      "Testing batch 202 loss: 0.019137805327773094\n",
      "Testing accuracy:  0.8527227722772277\n",
      "Testing batch 203 loss: 0.042683061212301254\n",
      "Testing accuracy:  0.8528325123152709\n",
      "Testing batch 204 loss: 0.0632367953658104\n",
      "Testing accuracy:  0.852328431372549\n",
      "Testing batch 205 loss: 0.040081579238176346\n",
      "Testing accuracy:  0.8524390243902439\n",
      "Testing batch 206 loss: 0.050177618861198425\n",
      "Testing accuracy:  0.8525485436893204\n",
      "Testing batch 207 loss: 0.042551472783088684\n",
      "Testing accuracy:  0.8526570048309179\n",
      "Testing batch 208 loss: 0.0301650557667017\n",
      "Testing accuracy:  0.8527644230769231\n",
      "Testing batch 209 loss: 0.045657332986593246\n",
      "Testing accuracy:  0.8528708133971292\n",
      "Testing batch 210 loss: 0.0776914730668068\n",
      "Testing accuracy:  0.8523809523809524\n",
      "Testing batch 211 loss: 0.03312188759446144\n",
      "Testing accuracy:  0.8530805687203792\n",
      "Testing batch 212 loss: 0.0369793102145195\n",
      "Testing accuracy:  0.8531839622641509\n",
      "Testing batch 213 loss: 0.03402414917945862\n",
      "Testing accuracy:  0.8538732394366197\n",
      "Testing batch 214 loss: 0.07800698280334473\n",
      "Testing accuracy:  0.8533878504672897\n",
      "Testing batch 215 loss: 0.08057253807783127\n",
      "Testing accuracy:  0.8534883720930233\n",
      "Testing batch 216 loss: 0.03010251186788082\n",
      "Testing accuracy:  0.8541666666666666\n",
      "Testing batch 217 loss: 0.06205703690648079\n",
      "Testing accuracy:  0.8542626728110599\n",
      "Testing batch 218 loss: 0.03640999644994736\n",
      "Testing accuracy:  0.8549311926605505\n",
      "Testing batch 219 loss: 0.03476574644446373\n",
      "Testing accuracy:  0.8550228310502284\n",
      "Testing batch 220 loss: 0.055042676627635956\n",
      "Testing accuracy:  0.8551136363636364\n",
      "Testing batch 221 loss: 0.05522090196609497\n",
      "Testing accuracy:  0.8546380090497737\n",
      "Testing batch 222 loss: 0.07179632782936096\n",
      "Testing accuracy:  0.8541666666666666\n",
      "Testing batch 223 loss: 0.019633106887340546\n",
      "Testing accuracy:  0.8548206278026906\n",
      "Testing batch 224 loss: 0.0592397078871727\n",
      "Testing accuracy:  0.8549107142857143\n",
      "Testing batch 225 loss: 0.07169786095619202\n",
      "Testing accuracy:  0.855\n",
      "Testing batch 226 loss: 0.043610624969005585\n",
      "Testing accuracy:  0.8545353982300885\n",
      "Testing batch 227 loss: 0.04230397567152977\n",
      "Testing accuracy:  0.8546255506607929\n",
      "Testing batch 228 loss: 0.0327838696539402\n",
      "Testing accuracy:  0.8552631578947368\n",
      "Testing batch 229 loss: 0.05819345638155937\n",
      "Testing accuracy:  0.855349344978166\n",
      "Testing batch 230 loss: 0.027476895600557327\n",
      "Testing accuracy:  0.8559782608695652\n",
      "Testing batch 231 loss: 0.025140423327684402\n",
      "Testing accuracy:  0.8566017316017316\n",
      "Testing batch 232 loss: 0.10741479694843292\n",
      "Testing accuracy:  0.8556034482758621\n",
      "Testing batch 233 loss: 0.06662901490926743\n",
      "Testing accuracy:  0.8546137339055794\n",
      "Testing batch 234 loss: 0.03870464116334915\n",
      "Testing accuracy:  0.8552350427350427\n",
      "Testing batch 235 loss: 0.03643115237355232\n",
      "Testing accuracy:  0.8558510638297873\n",
      "Testing batch 236 loss: 0.10349142551422119\n",
      "Testing accuracy:  0.8548728813559322\n",
      "Testing batch 237 loss: 0.06036103144288063\n",
      "Testing accuracy:  0.8544303797468354\n",
      "Testing batch 238 loss: 0.08573628216981888\n",
      "Testing accuracy:  0.8539915966386554\n",
      "Testing batch 239 loss: 0.03705960512161255\n",
      "Testing accuracy:  0.8540794979079498\n",
      "Testing batch 240 loss: 0.04327503964304924\n",
      "Testing accuracy:  0.8541666666666666\n",
      "Testing batch 241 loss: 0.08897043019533157\n",
      "Testing accuracy:  0.8537344398340249\n",
      "Testing batch 242 loss: 0.07855039834976196\n",
      "Testing accuracy:  0.8527892561983471\n",
      "Testing batch 243 loss: 0.07995138317346573\n",
      "Testing accuracy:  0.852366255144033\n",
      "Testing batch 244 loss: 0.017413461580872536\n",
      "Testing accuracy:  0.8529713114754098\n",
      "Testing batch 245 loss: 0.03454769402742386\n",
      "Testing accuracy:  0.8535714285714285\n",
      "Testing batch 246 loss: 0.03815479949116707\n",
      "Testing accuracy:  0.8536585365853658\n",
      "Testing batch 247 loss: 0.06128884479403496\n",
      "Testing accuracy:  0.853744939271255\n",
      "Testing batch 248 loss: 0.03744540736079216\n",
      "Testing accuracy:  0.8543346774193549\n",
      "Testing batch 249 loss: 0.10014350712299347\n",
      "Testing accuracy:  0.8539156626506024\n",
      "Testing batch 250 loss: 0.023602403700351715\n",
      "Testing accuracy:  0.8545\n",
      "Testing batch 251 loss: 0.016714079305529594\n",
      "Testing accuracy:  0.8550796812749004\n",
      "Testing batch 252 loss: 0.07161680608987808\n",
      "Testing accuracy:  0.8546626984126984\n",
      "Testing batch 253 loss: 0.07109670341014862\n",
      "Testing accuracy:  0.8547430830039525\n",
      "Testing batch 254 loss: 0.04904203861951828\n",
      "Testing accuracy:  0.8543307086614174\n",
      "Testing batch 255 loss: 0.0671272873878479\n",
      "Testing accuracy:  0.8544117647058823\n",
      "Testing batch 256 loss: 0.030841343104839325\n",
      "Testing accuracy:  0.8544921875\n",
      "Testing batch 257 loss: 0.03477735072374344\n",
      "Testing accuracy:  0.8545719844357976\n",
      "Testing batch 258 loss: 0.052881985902786255\n",
      "Testing accuracy:  0.8541666666666666\n",
      "Testing batch 259 loss: 0.10904812812805176\n",
      "Testing accuracy:  0.8537644787644788\n",
      "Testing batch 260 loss: 0.026798035949468613\n",
      "Testing accuracy:  0.854326923076923\n",
      "Testing batch 261 loss: 0.03493763878941536\n",
      "Testing accuracy:  0.8544061302681992\n",
      "Testing batch 262 loss: 0.040549151599407196\n",
      "Testing accuracy:  0.8549618320610687\n",
      "Testing batch 263 loss: 0.027408327907323837\n",
      "Testing accuracy:  0.8555133079847909\n",
      "Testing batch 264 loss: 0.021127279847860336\n",
      "Testing accuracy:  0.8560606060606061\n",
      "Testing batch 265 loss: 0.059258345514535904\n",
      "Testing accuracy:  0.855188679245283\n",
      "Testing batch 266 loss: 0.058531563729047775\n",
      "Testing accuracy:  0.8547932330827067\n",
      "Testing batch 267 loss: 0.03966793417930603\n",
      "Testing accuracy:  0.8548689138576779\n",
      "Testing batch 268 loss: 0.04416457563638687\n",
      "Testing accuracy:  0.8549440298507462\n",
      "Testing batch 269 loss: 0.04173234850168228\n",
      "Testing accuracy:  0.8554832713754646\n",
      "Testing batch 270 loss: 0.030350442975759506\n",
      "Testing accuracy:  0.8555555555555555\n",
      "Testing batch 271 loss: 0.029976995661854744\n",
      "Testing accuracy:  0.8560885608856088\n",
      "Testing batch 272 loss: 0.013579967431724072\n",
      "Testing accuracy:  0.8566176470588235\n",
      "Testing batch 273 loss: 0.0473950058221817\n",
      "Testing accuracy:  0.8566849816849816\n",
      "Testing batch 274 loss: 0.048264458775520325\n",
      "Testing accuracy:  0.8562956204379562\n",
      "Testing batch 275 loss: 0.047836076468229294\n",
      "Testing accuracy:  0.855909090909091\n",
      "Testing batch 276 loss: 0.022876987233757973\n",
      "Testing accuracy:  0.8564311594202898\n",
      "Testing batch 277 loss: 0.04076103866100311\n",
      "Testing accuracy:  0.8564981949458483\n",
      "Testing batch 278 loss: 0.04871150106191635\n",
      "Testing accuracy:  0.8565647482014388\n",
      "Testing batch 279 loss: 0.13016949594020844\n",
      "Testing accuracy:  0.8561827956989247\n",
      "Testing batch 280 loss: 0.06666091829538345\n",
      "Testing accuracy:  0.85625\n",
      "Testing batch 281 loss: 0.09390129894018173\n",
      "Testing accuracy:  0.8554270462633452\n",
      "Testing batch 282 loss: 0.04367002472281456\n",
      "Testing accuracy:  0.8554964539007093\n",
      "Testing batch 283 loss: 0.03558607026934624\n",
      "Testing accuracy:  0.855565371024735\n",
      "Testing batch 284 loss: 0.08009468764066696\n",
      "Testing accuracy:  0.855193661971831\n",
      "Testing batch 285 loss: 0.08240678906440735\n",
      "Testing accuracy:  0.8543859649122807\n",
      "Testing batch 286 loss: 0.07336370646953583\n",
      "Testing accuracy:  0.854458041958042\n",
      "Testing batch 287 loss: 0.03982849791646004\n",
      "Testing accuracy:  0.8549651567944251\n",
      "Testing batch 288 loss: 0.0186298917979002\n",
      "Testing accuracy:  0.85546875\n",
      "Testing batch 289 loss: 0.08860552310943604\n",
      "Testing accuracy:  0.8546712802768166\n",
      "Testing batch 290 loss: 0.03594125807285309\n",
      "Testing accuracy:  0.8547413793103448\n",
      "Testing batch 291 loss: 0.10150104016065598\n",
      "Testing accuracy:  0.8539518900343642\n",
      "Testing batch 292 loss: 0.03180335834622383\n",
      "Testing accuracy:  0.8540239726027398\n",
      "Testing batch 293 loss: 0.04633486270904541\n",
      "Testing accuracy:  0.8536689419795221\n",
      "Testing batch 294 loss: 0.02706066332757473\n",
      "Testing accuracy:  0.8537414965986394\n",
      "Testing batch 295 loss: 0.08591966331005096\n",
      "Testing accuracy:  0.8533898305084746\n",
      "Testing batch 296 loss: 0.02876080945134163\n",
      "Testing accuracy:  0.8538851351351351\n",
      "Testing batch 297 loss: 0.03168434649705887\n",
      "Testing accuracy:  0.8543771043771043\n",
      "Testing batch 298 loss: 0.03670826554298401\n",
      "Testing accuracy:  0.8544463087248322\n",
      "Testing batch 299 loss: 0.042107902467250824\n",
      "Testing accuracy:  0.8545150501672241\n",
      "Testing batch 300 loss: 0.047659218311309814\n",
      "Testing accuracy:  0.8541666666666666\n",
      "Testing batch 301 loss: 0.041248876601457596\n",
      "Testing accuracy:  0.8546511627906976\n",
      "Testing batch 302 loss: 0.05141350254416466\n",
      "Testing accuracy:  0.8547185430463576\n",
      "Testing batch 303 loss: 0.05476076155900955\n",
      "Testing accuracy:  0.8547854785478548\n",
      "Testing batch 304 loss: 0.029125597327947617\n",
      "Testing accuracy:  0.8552631578947368\n",
      "Testing batch 305 loss: 0.027045689523220062\n",
      "Testing accuracy:  0.8557377049180328\n",
      "Testing batch 306 loss: 0.08727138489484787\n",
      "Testing accuracy:  0.8553921568627451\n",
      "Testing batch 307 loss: 0.12044456601142883\n",
      "Testing accuracy:  0.8538273615635179\n",
      "Testing batch 308 loss: 0.07550668716430664\n",
      "Testing accuracy:  0.8538961038961039\n",
      "Testing batch 309 loss: 0.03367643803358078\n",
      "Testing accuracy:  0.8539644012944984\n",
      "Testing batch 310 loss: 0.06445250660181046\n",
      "Testing accuracy:  0.8540322580645161\n",
      "Testing batch 311 loss: 0.06092390790581703\n",
      "Testing accuracy:  0.8540996784565916\n",
      "Testing batch 312 loss: 0.082088902592659\n",
      "Testing accuracy:  0.8529647435897436\n",
      "Testing batch 313 loss: 0.05220251530408859\n",
      "Testing accuracy:  0.8526357827476039\n",
      "Testing batch 314 loss: 0.04299655556678772\n",
      "Testing accuracy:  0.8523089171974523\n",
      "Testing batch 315 loss: 0.04057375341653824\n",
      "Testing accuracy:  0.8527777777777777\n",
      "Testing batch 316 loss: 0.06921499967575073\n",
      "Testing accuracy:  0.8528481012658228\n",
      "Testing batch 317 loss: 0.06511517614126205\n",
      "Testing accuracy:  0.8525236593059937\n",
      "Testing batch 318 loss: 0.03146253898739815\n",
      "Testing accuracy:  0.8525943396226415\n",
      "Testing batch 319 loss: 0.07827308028936386\n",
      "Testing accuracy:  0.8522727272727273\n",
      "Testing batch 320 loss: 0.06757361441850662\n",
      "Testing accuracy:  0.85234375\n",
      "Testing batch 321 loss: 0.03919589892029762\n",
      "Testing accuracy:  0.8524143302180686\n",
      "Testing batch 322 loss: 0.05782344564795494\n",
      "Testing accuracy:  0.8517080745341615\n",
      "Testing batch 323 loss: 0.03638724237680435\n",
      "Testing accuracy:  0.8521671826625387\n",
      "Testing batch 324 loss: 0.05947880819439888\n",
      "Testing accuracy:  0.8522376543209876\n",
      "Testing batch 325 loss: 0.09977027773857117\n",
      "Testing accuracy:  0.8515384615384616\n",
      "Testing batch 326 loss: 0.1025959774851799\n",
      "Testing accuracy:  0.8504601226993865\n",
      "Testing batch 327 loss: 0.08552375435829163\n",
      "Testing accuracy:  0.8501529051987767\n",
      "Testing batch 328 loss: 0.038509488105773926\n",
      "Testing accuracy:  0.850609756097561\n",
      "Testing batch 329 loss: 0.033975645899772644\n",
      "Testing accuracy:  0.8506838905775076\n",
      "Testing batch 330 loss: 0.0937381237745285\n",
      "Testing accuracy:  0.85\n",
      "Testing batch 331 loss: 0.05137808248400688\n",
      "Testing accuracy:  0.8496978851963746\n",
      "Testing batch 332 loss: 0.05643713101744652\n",
      "Testing accuracy:  0.8497740963855421\n",
      "Testing batch 333 loss: 0.03975333273410797\n",
      "Testing accuracy:  0.8502252252252253\n",
      "Testing batch 334 loss: 0.06644347310066223\n",
      "Testing accuracy:  0.8495508982035929\n",
      "Testing batch 335 loss: 0.034394655376672745\n",
      "Testing accuracy:  0.8496268656716418\n",
      "Testing batch 336 loss: 0.039394304156303406\n",
      "Testing accuracy:  0.8497023809523809\n",
      "Testing batch 337 loss: 0.04001069813966751\n",
      "Testing accuracy:  0.8497774480712166\n",
      "Testing batch 338 loss: 0.06373143941164017\n",
      "Testing accuracy:  0.8494822485207101\n",
      "Testing batch 339 loss: 0.027786724269390106\n",
      "Testing accuracy:  0.8495575221238938\n",
      "Testing batch 340 loss: 0.10429571568965912\n",
      "Testing accuracy:  0.8488970588235294\n",
      "Testing batch 341 loss: 0.021705660969018936\n",
      "Testing accuracy:  0.8489736070381232\n",
      "Testing batch 342 loss: 0.08746309578418732\n",
      "Testing accuracy:  0.8490497076023392\n",
      "Testing batch 343 loss: 0.04010577127337456\n",
      "Testing accuracy:  0.8491253644314869\n",
      "Testing batch 344 loss: 0.027282118797302246\n",
      "Testing accuracy:  0.8495639534883721\n",
      "Testing batch 345 loss: 0.0946849137544632\n",
      "Testing accuracy:  0.8492753623188406\n",
      "Testing batch 346 loss: 0.054942596703767776\n",
      "Testing accuracy:  0.849349710982659\n",
      "Testing batch 347 loss: 0.021595144644379616\n",
      "Testing accuracy:  0.8497838616714697\n",
      "Testing batch 348 loss: 0.019113468006253242\n",
      "Testing accuracy:  0.8502155172413793\n",
      "Testing batch 349 loss: 0.059872452169656754\n",
      "Testing accuracy:  0.8502865329512894\n",
      "Testing batch 350 loss: 0.03461165353655815\n",
      "Testing accuracy:  0.8503571428571428\n",
      "Testing batch 351 loss: 0.03943156823515892\n",
      "Testing accuracy:  0.8507834757834758\n",
      "Testing batch 352 loss: 0.04417899250984192\n",
      "Testing accuracy:  0.8508522727272727\n",
      "Testing batch 353 loss: 0.09104125946760178\n",
      "Testing accuracy:  0.8505665722379604\n",
      "Testing batch 354 loss: 0.10119298100471497\n",
      "Testing accuracy:  0.8499293785310734\n",
      "Testing batch 355 loss: 0.032806724309921265\n",
      "Testing accuracy:  0.8503521126760564\n",
      "Testing batch 356 loss: 0.04369206726551056\n",
      "Testing accuracy:  0.8504213483146067\n",
      "Testing batch 357 loss: 0.06606942415237427\n",
      "Testing accuracy:  0.8494397759103641\n",
      "Testing batch 358 loss: 0.06596899032592773\n",
      "Testing accuracy:  0.8491620111731844\n",
      "Testing batch 359 loss: 0.1210121214389801\n",
      "Testing accuracy:  0.8485376044568245\n",
      "Testing batch 360 loss: 0.020971626043319702\n",
      "Testing accuracy:  0.8489583333333334\n",
      "Testing batch 361 loss: 0.04877355694770813\n",
      "Testing accuracy:  0.8490304709141274\n",
      "Testing batch 362 loss: 0.04126785323023796\n",
      "Testing accuracy:  0.8487569060773481\n",
      "Testing batch 363 loss: 0.02455928549170494\n",
      "Testing accuracy:  0.8491735537190083\n",
      "Testing batch 364 loss: 0.03757366165518761\n",
      "Testing accuracy:  0.8495879120879121\n",
      "Testing batch 365 loss: 0.04574492201209068\n",
      "Testing accuracy:  0.8496575342465753\n",
      "Testing batch 366 loss: 0.04714702442288399\n",
      "Testing accuracy:  0.850068306010929\n",
      "Testing batch 367 loss: 0.03760230541229248\n",
      "Testing accuracy:  0.8501362397820164\n",
      "Testing batch 368 loss: 0.01983151212334633\n",
      "Testing accuracy:  0.8505434782608695\n",
      "Testing batch 369 loss: 0.054657500237226486\n",
      "Testing accuracy:  0.850609756097561\n",
      "Testing batch 370 loss: 0.027835311368107796\n",
      "Testing accuracy:  0.8510135135135135\n",
      "Testing batch 371 loss: 0.040936507284641266\n",
      "Testing accuracy:  0.8514150943396226\n",
      "Testing batch 372 loss: 0.025893904268741608\n",
      "Testing accuracy:  0.8518145161290323\n",
      "Testing batch 373 loss: 0.053338367491960526\n",
      "Testing accuracy:  0.8518766756032171\n",
      "Testing batch 374 loss: 0.03399423509836197\n",
      "Testing accuracy:  0.8519385026737968\n",
      "Testing batch 375 loss: 0.04395519196987152\n",
      "Testing accuracy:  0.8516666666666667\n",
      "Testing batch 376 loss: 0.05126342549920082\n",
      "Testing accuracy:  0.8513962765957447\n",
      "Testing batch 377 loss: 0.033128831535577774\n",
      "Testing accuracy:  0.851790450928382\n",
      "Testing batch 378 loss: 0.0912204384803772\n",
      "Testing accuracy:  0.8511904761904762\n",
      "Testing batch 379 loss: 0.07338286191225052\n",
      "Testing accuracy:  0.8512532981530343\n",
      "Testing batch 380 loss: 0.050586242228746414\n",
      "Testing accuracy:  0.8513157894736842\n",
      "Testing batch 381 loss: 0.049188774079084396\n",
      "Testing accuracy:  0.8510498687664042\n",
      "Testing batch 382 loss: 0.025716254487633705\n",
      "Testing accuracy:  0.8514397905759162\n",
      "Testing batch 383 loss: 0.03660379350185394\n",
      "Testing accuracy:  0.8518276762402088\n",
      "Testing batch 384 loss: 0.04805528372526169\n",
      "Testing accuracy:  0.8515625\n",
      "Testing batch 385 loss: 0.02527783066034317\n",
      "Testing accuracy:  0.8519480519480519\n",
      "Testing batch 386 loss: 0.06087851524353027\n",
      "Testing accuracy:  0.8516839378238342\n",
      "Testing batch 387 loss: 0.06497765332460403\n",
      "Testing accuracy:  0.851421188630491\n",
      "Testing batch 388 loss: 0.051875680685043335\n",
      "Testing accuracy:  0.8514819587628866\n",
      "Testing batch 389 loss: 0.030292464420199394\n",
      "Testing accuracy:  0.8518637532133676\n",
      "Testing batch 390 loss: 0.05932217091321945\n",
      "Testing accuracy:  0.8516025641025641\n",
      "Testing batch 391 loss: 0.10086517781019211\n",
      "Testing accuracy:  0.8510230179028133\n",
      "Testing batch 392 loss: 0.09299428761005402\n",
      "Testing accuracy:  0.8504464285714286\n",
      "Testing batch 393 loss: 0.041237033903598785\n",
      "Testing accuracy:  0.8505089058524173\n",
      "Testing batch 394 loss: 0.028033744543790817\n",
      "Testing accuracy:  0.8508883248730964\n",
      "Testing batch 395 loss: 0.05297806113958359\n",
      "Testing accuracy:  0.850632911392405\n",
      "Testing batch 396 loss: 0.04762826859951019\n",
      "Testing accuracy:  0.8506944444444444\n",
      "Testing batch 397 loss: 0.026002641767263412\n",
      "Testing accuracy:  0.8510705289672544\n",
      "Testing batch 398 loss: 0.048977892845869064\n",
      "Testing accuracy:  0.8511306532663316\n",
      "Testing batch 399 loss: 0.04445100203156471\n",
      "Testing accuracy:  0.8511904761904762\n",
      "Testing batch 400 loss: 0.05083885416388512\n",
      "Testing accuracy:  0.8509375\n",
      "Testing batch 401 loss: 0.047290921211242676\n",
      "Testing accuracy:  0.850997506234414\n",
      "Testing batch 402 loss: 0.030952945351600647\n",
      "Testing accuracy:  0.8510572139303483\n",
      "Testing batch 403 loss: 0.04474097490310669\n",
      "Testing accuracy:  0.8514267990074442\n",
      "Testing batch 404 loss: 0.0495365634560585\n",
      "Testing accuracy:  0.8514851485148515\n",
      "Testing batch 405 loss: 0.05190052092075348\n",
      "Testing accuracy:  0.8512345679012345\n",
      "Testing batch 406 loss: 0.0684046596288681\n",
      "Testing accuracy:  0.8509852216748769\n",
      "Testing batch 407 loss: 0.026221221312880516\n",
      "Testing accuracy:  0.8513513513513513\n",
      "Testing batch 408 loss: 0.03372938185930252\n",
      "Testing accuracy:  0.8514093137254902\n",
      "Testing batch 409 loss: 0.04132016748189926\n",
      "Testing accuracy:  0.8514669926650367\n",
      "Testing batch 410 loss: 0.07175806909799576\n",
      "Testing accuracy:  0.8512195121951219\n",
      "Testing batch 411 loss: 0.022359376773238182\n",
      "Testing accuracy:  0.851581508515815\n",
      "Testing batch 412 loss: 0.0592380091547966\n",
      "Testing accuracy:  0.8516383495145631\n",
      "Testing batch 413 loss: 0.018246645107865334\n",
      "Testing accuracy:  0.8519975786924939\n",
      "Testing batch 414 loss: 0.04378477483987808\n",
      "Testing accuracy:  0.8520531400966184\n",
      "Testing batch 415 loss: 0.051049135625362396\n",
      "Testing accuracy:  0.8518072289156626\n",
      "Testing batch 416 loss: 0.06701911985874176\n",
      "Testing accuracy:  0.8515625\n",
      "Testing batch 417 loss: 0.033297859132289886\n",
      "Testing accuracy:  0.8519184652278178\n",
      "Testing batch 418 loss: 0.03334401175379753\n",
      "Testing accuracy:  0.8519736842105263\n",
      "Testing batch 419 loss: 0.041022300720214844\n",
      "Testing accuracy:  0.8517303102625299\n",
      "Testing batch 420 loss: 0.04072505235671997\n",
      "Testing accuracy:  0.8517857142857143\n",
      "Testing batch 421 loss: 0.021108919754624367\n",
      "Testing accuracy:  0.8521377672209026\n",
      "Testing batch 422 loss: 0.02838602289557457\n",
      "Testing accuracy:  0.8524881516587678\n",
      "Testing batch 423 loss: 0.026736672967672348\n",
      "Testing accuracy:  0.8528368794326241\n",
      "Testing batch 424 loss: 0.033396705985069275\n",
      "Testing accuracy:  0.8528891509433962\n",
      "Testing batch 425 loss: 0.056374721229076385\n",
      "Testing accuracy:  0.8529411764705882\n",
      "Testing batch 426 loss: 0.09921256452798843\n",
      "Testing accuracy:  0.8526995305164319\n",
      "Testing batch 427 loss: 0.05972443148493767\n",
      "Testing accuracy:  0.8524590163934426\n",
      "Testing batch 428 loss: 0.035131145268678665\n",
      "Testing accuracy:  0.8525116822429907\n",
      "Testing batch 429 loss: 0.038260869681835175\n",
      "Testing accuracy:  0.8525641025641025\n",
      "Testing batch 430 loss: 0.06568530201911926\n",
      "Testing accuracy:  0.8523255813953489\n",
      "Testing batch 431 loss: 0.041799455881118774\n",
      "Testing accuracy:  0.8523781902552204\n",
      "Testing batch 432 loss: 0.08747228980064392\n",
      "Testing accuracy:  0.8521412037037037\n",
      "Testing batch 433 loss: 0.025076324120163918\n",
      "Testing accuracy:  0.8524826789838337\n",
      "Testing batch 434 loss: 0.02150246873497963\n",
      "Testing accuracy:  0.8528225806451613\n",
      "Testing batch 435 loss: 0.018464649096131325\n",
      "Testing accuracy:  0.8531609195402299\n",
      "Testing batch 436 loss: 0.05746104568243027\n",
      "Testing accuracy:  0.8529243119266054\n",
      "Testing batch 437 loss: 0.021054130047559738\n",
      "Testing accuracy:  0.8532608695652174\n",
      "Testing batch 438 loss: 0.04146324470639229\n",
      "Testing accuracy:  0.853595890410959\n",
      "Testing batch 439 loss: 0.03777269273996353\n",
      "Testing accuracy:  0.8536446469248291\n",
      "Testing batch 440 loss: 0.033662810921669006\n",
      "Testing accuracy:  0.8539772727272728\n",
      "Testing batch 441 loss: 0.07322641462087631\n",
      "Testing accuracy:  0.8537414965986394\n",
      "Testing batch 442 loss: 0.03475800156593323\n",
      "Testing accuracy:  0.8540723981900452\n",
      "Testing batch 443 loss: 0.02613932453095913\n",
      "Testing accuracy:  0.8544018058690744\n",
      "Testing batch 444 loss: 0.07615279406309128\n",
      "Testing accuracy:  0.8538851351351351\n",
      "Testing batch 445 loss: 0.03646286949515343\n",
      "Testing accuracy:  0.8542134831460674\n",
      "Testing batch 446 loss: 0.04125477746129036\n",
      "Testing accuracy:  0.8542600896860987\n",
      "Testing batch 447 loss: 0.032864831387996674\n",
      "Testing accuracy:  0.8543064876957495\n",
      "Testing batch 448 loss: 0.02431330643594265\n",
      "Testing accuracy:  0.8546316964285714\n",
      "Testing batch 449 loss: 0.10460131615400314\n",
      "Testing accuracy:  0.8543986636971047\n",
      "Testing batch 450 loss: 0.03491567075252533\n",
      "Testing accuracy:  0.8544444444444445\n",
      "Testing batch 451 loss: 0.027829818427562714\n",
      "Testing accuracy:  0.854490022172949\n",
      "Testing batch 452 loss: 0.030330032110214233\n",
      "Testing accuracy:  0.8548119469026548\n",
      "Testing batch 453 loss: 0.03086601011455059\n",
      "Testing accuracy:  0.8551324503311258\n",
      "Testing batch 454 loss: 0.019744999706745148\n",
      "Testing accuracy:  0.8554515418502202\n",
      "Testing batch 455 loss: 0.04962819069623947\n",
      "Testing accuracy:  0.8552197802197802\n",
      "Testing batch 456 loss: 0.056611571460962296\n",
      "Testing accuracy:  0.8549890350877193\n",
      "Testing batch 457 loss: 0.05865319073200226\n",
      "Testing accuracy:  0.8547592997811816\n",
      "Testing batch 458 loss: 0.07235205918550491\n",
      "Testing accuracy:  0.8545305676855895\n",
      "Testing batch 459 loss: 0.05294904485344887\n",
      "Testing accuracy:  0.8543028322440087\n",
      "Testing batch 460 loss: 0.10332920402288437\n",
      "Testing accuracy:  0.8535326086956522\n",
      "Testing batch 461 loss: 0.08106448501348495\n",
      "Testing accuracy:  0.8530368763557483\n",
      "Testing batch 462 loss: 0.05481526255607605\n",
      "Testing accuracy:  0.8530844155844156\n",
      "Testing batch 463 loss: 0.06652070581912994\n",
      "Testing accuracy:  0.8525917926565875\n",
      "Testing batch 464 loss: 0.03664751723408699\n",
      "Testing accuracy:  0.8526400862068966\n",
      "Testing batch 465 loss: 0.0363716259598732\n",
      "Testing accuracy:  0.8526881720430107\n",
      "Testing batch 466 loss: 0.025484859943389893\n",
      "Testing accuracy:  0.8530042918454935\n",
      "Testing batch 467 loss: 0.03353026136755943\n",
      "Testing accuracy:  0.853051391862955\n",
      "Testing batch 468 loss: 0.03373795747756958\n",
      "Testing accuracy:  0.8530982905982906\n",
      "Testing batch 469 loss: 0.09942165017127991\n",
      "Testing accuracy:  0.8528784648187633\n",
      "Testing batch 470 loss: 0.03588086739182472\n",
      "Testing accuracy:  0.8531914893617021\n",
      "Testing batch 471 loss: 0.01663946360349655\n",
      "Testing accuracy:  0.8535031847133758\n",
      "Testing batch 472 loss: 0.03858129680156708\n",
      "Testing accuracy:  0.8538135593220338\n",
      "Testing batch 473 loss: 0.025225268676877022\n",
      "Testing accuracy:  0.854122621564482\n",
      "Testing batch 474 loss: 0.023753834888339043\n",
      "Testing accuracy:  0.8544303797468354\n",
      "Testing batch 475 loss: 0.035726916044950485\n",
      "Testing accuracy:  0.8547368421052631\n",
      "Testing batch 476 loss: 0.04357236251235008\n",
      "Testing accuracy:  0.8547794117647058\n",
      "Testing batch 477 loss: 0.03508502617478371\n",
      "Testing accuracy:  0.855083857442348\n",
      "Testing batch 478 loss: 0.011895887553691864\n",
      "Testing accuracy:  0.8553870292887029\n",
      "Testing batch 479 loss: 0.04274231195449829\n",
      "Testing accuracy:  0.8551670146137788\n",
      "Testing batch 480 loss: 0.06734726577997208\n",
      "Testing accuracy:  0.8552083333333333\n",
      "Testing batch 481 loss: 0.04348690062761307\n",
      "Testing accuracy:  0.8552494802494802\n",
      "Testing batch 482 loss: 0.028235863894224167\n",
      "Testing accuracy:  0.8552904564315352\n",
      "Testing batch 483 loss: 0.05957760289311409\n",
      "Testing accuracy:  0.8548136645962733\n",
      "Testing batch 484 loss: 0.04803375154733658\n",
      "Testing accuracy:  0.8545971074380165\n",
      "Testing batch 485 loss: 0.06965352594852448\n",
      "Testing accuracy:  0.8541237113402061\n",
      "Testing batch 486 loss: 0.04785117879509926\n",
      "Testing accuracy:  0.8541666666666666\n",
      "Testing batch 487 loss: 0.05950695648789406\n",
      "Testing accuracy:  0.853952772073922\n",
      "Testing batch 488 loss: 0.04437299445271492\n",
      "Testing accuracy:  0.8539959016393442\n",
      "Testing batch 489 loss: 0.05133502930402756\n",
      "Testing accuracy:  0.8537832310838446\n",
      "Testing batch 490 loss: 0.04748871549963951\n",
      "Testing accuracy:  0.8538265306122449\n",
      "Testing batch 491 loss: 0.07423564791679382\n",
      "Testing accuracy:  0.8533604887983707\n",
      "Testing batch 492 loss: 0.042044855654239655\n",
      "Testing accuracy:  0.8534044715447154\n",
      "Testing batch 493 loss: 0.05735790729522705\n",
      "Testing accuracy:  0.8531947261663286\n",
      "Testing batch 494 loss: 0.06265997886657715\n",
      "Testing accuracy:  0.8529858299595142\n",
      "Testing batch 495 loss: 0.04085919260978699\n",
      "Testing accuracy:  0.853030303030303\n",
      "Testing batch 496 loss: 0.042098212987184525\n",
      "Testing accuracy:  0.8528225806451613\n",
      "Testing batch 497 loss: 0.07043491303920746\n",
      "Testing accuracy:  0.85261569416499\n",
      "Testing batch 498 loss: 0.04479926824569702\n",
      "Testing accuracy:  0.8526606425702812\n",
      "Testing batch 499 loss: 0.035946644842624664\n",
      "Testing accuracy:  0.8529559118236473\n",
      "Testing batch 500 loss: 0.10446091741323471\n",
      "Testing accuracy:  0.85225\n",
      "Testing batch 501 loss: 0.03467048704624176\n",
      "Testing accuracy:  0.8525449101796407\n",
      "Testing batch 502 loss: 0.04562591388821602\n",
      "Testing accuracy:  0.8528386454183267\n",
      "Testing batch 503 loss: 0.038522399961948395\n",
      "Testing accuracy:  0.852882703777336\n",
      "Testing batch 504 loss: 0.039722271263599396\n",
      "Testing accuracy:  0.8529265873015873\n",
      "Testing batch 505 loss: 0.04733533784747124\n",
      "Testing accuracy:  0.852970297029703\n",
      "Testing batch 506 loss: 0.04610929638147354\n",
      "Testing accuracy:  0.8530138339920948\n",
      "Testing batch 507 loss: 0.03469380736351013\n",
      "Testing accuracy:  0.8530571992110454\n",
      "Testing batch 508 loss: 0.043642979115247726\n",
      "Testing accuracy:  0.8531003937007874\n",
      "Testing batch 509 loss: 0.030775737017393112\n",
      "Testing accuracy:  0.8533889980353635\n",
      "Testing batch 510 loss: 0.04149794206023216\n",
      "Testing accuracy:  0.8536764705882353\n",
      "Testing batch 511 loss: 0.06485474854707718\n",
      "Testing accuracy:  0.8532289628180039\n",
      "Testing batch 512 loss: 0.031827088445425034\n",
      "Testing accuracy:  0.853515625\n",
      "Testing batch 513 loss: 0.08944499492645264\n",
      "Testing accuracy:  0.8533138401559455\n",
      "Testing batch 514 loss: 0.05067731812596321\n",
      "Testing accuracy:  0.8528696498054474\n",
      "Testing batch 515 loss: 0.06356444954872131\n",
      "Testing accuracy:  0.8524271844660194\n",
      "Testing batch 516 loss: 0.030943691730499268\n",
      "Testing accuracy:  0.8527131782945736\n",
      "Testing batch 517 loss: 0.030164463445544243\n",
      "Testing accuracy:  0.8527562862669246\n",
      "Testing batch 518 loss: 0.02596818096935749\n",
      "Testing accuracy:  0.8530405405405406\n",
      "Testing batch 519 loss: 0.08614353835582733\n",
      "Testing accuracy:  0.8523603082851637\n",
      "Testing batch 520 loss: 0.024504438042640686\n",
      "Testing accuracy:  0.8526442307692308\n",
      "Testing batch 521 loss: 0.03041483275592327\n",
      "Testing accuracy:  0.8529270633397313\n",
      "Testing batch 522 loss: 0.06281011551618576\n",
      "Testing accuracy:  0.8527298850574713\n",
      "Testing batch 523 loss: 0.04984503239393234\n",
      "Testing accuracy:  0.8525334608030593\n",
      "Testing batch 524 loss: 0.030199479311704636\n",
      "Testing accuracy:  0.8528148854961832\n",
      "Testing batch 525 loss: 0.06067856773734093\n",
      "Testing accuracy:  0.8523809523809524\n",
      "Testing batch 526 loss: 0.02089770697057247\n",
      "Testing accuracy:  0.8526615969581749\n",
      "Testing batch 527 loss: 0.039596375077962875\n",
      "Testing accuracy:  0.8527039848197343\n",
      "Testing batch 528 loss: 0.05275222659111023\n",
      "Testing accuracy:  0.8525094696969697\n",
      "Testing batch 529 loss: 0.023376688361167908\n",
      "Testing accuracy:  0.8523156899810964\n",
      "\n",
      "Testing epoch 1 last loss:  0.023376688361167908\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch: \",(epoch + 1))\n",
    "    # Sets the model to train mode.\n",
    "    bert_model.train()\n",
    "    # Here, we iterate over each batch in the train_loader dataset.\n",
    "    for i,batch in enumerate(train_loader):\n",
    "        # Move each batch to the CPU.\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        # We reset the gradients from the previous step before setting them for the current step.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # <-- TODO --->\n",
    "\n",
    "        outputs = bert_model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "        pred = outputs.logits\n",
    "        loss = loss_fn(pred, batch['labels'])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # <------------>\n",
    "\n",
    "        # Calculating the running loss for logging purposes\n",
    "        train_batch_loss = loss.item()\n",
    "        train_last_loss = train_batch_loss / batch_size\n",
    "        print('Training batch {} last loss: {}'.format(i + 1, train_last_loss))\n",
    "\n",
    "    # Logging epoch-wise training loss\n",
    "    print(f\"\\nTraining epoch {epoch + 1} loss: \",train_last_loss)\n",
    "    # TRAINING BLOCK ENDS\n",
    "\n",
    "    # Set the model to eval() mode.\n",
    "    bert_model.eval()\n",
    "    correct = 0\n",
    "    test_pred = []\n",
    "\n",
    "    # Testing the accuracy of our code on the test data\n",
    "    for i, batch in enumerate(test_loader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        # We don't need gradients for testing\n",
    "        with torch.no_grad():\n",
    "            outputs = bert_model(input_ids = batch['input_ids'], attention_mask = batch['attention_mask'])\n",
    "\n",
    "        # <!-- TODO -->\n",
    "\n",
    "        logits = outputs.logits\n",
    "        loss = loss_fn(logits, batch['labels'])\n",
    "        test_batch_loss = loss.item()\n",
    "        test_last_loss = test_batch_loss / batch_size\n",
    "\n",
    "        # <----------->\n",
    "        print('Testing batch {} loss: {}'.format(i + 1, test_last_loss))\n",
    "\n",
    "        # Comparing the predicted target with the labels in the batch\n",
    "        correct += (logits.argmax(1) == batch['labels']).sum().item()\n",
    "        print(\"Testing accuracy: \",correct/((i + 1) * batch_size))\n",
    "\n",
    "    print(f\"\\nTesting epoch {epoch + 1} last loss: \",test_last_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ed3e53-48e8-4776-969f-9aec7b5ac569",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "id": "f4ed3e53-48e8-4776-969f-9aec7b5ac569",
    "outputId": "321e1e73-b491-4ed6-cec3-42a0c27e2734"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bert_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a955849816b4>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Replace this with your input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"The effects can still be felt today\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# If the class is 0, it is not COVID related and if it is 1, then it is COVID related.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bert_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Define a function to predict a new text input\n",
    "def predict(text, model, tokenizer):\n",
    "    # Tokenize the text (make sure to use padding and truncation)\n",
    "    tokens = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "\n",
    "    # Move tokens to the same device as the model\n",
    "    tokens = {k: v.to(device) for k, v in tokens.items()}\n",
    "\n",
    "    # Put model in evaluation mode and make prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "\n",
    "    # Extract logits and find the predicted class\n",
    "    logits = outputs.logits\n",
    "    predicted_class = logits.argmax(dim=1).item()\n",
    "\n",
    "    # Return predicted class\n",
    "    return predicted_class\n",
    "\n",
    "# Replace this with your input\n",
    "text = \"The effects can still be felt today\"\n",
    "prediction = predict(text, bert_model, tokenizer)\n",
    "\n",
    "# If the class is 0, it is not COVID related and if it is 1, then it is COVID related.\n",
    "print(\"Predicted class:\", prediction)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
